# -*- coding: utf-8 -*-
"""Regression Servers

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WLkSiQ69LcKoGdzDUa7-9xIZ-sS_-6NW
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn import metrics
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn import linear_model
import pickle

#url = 'https://raw.githubusercontent.com/julianvanmeggelen/AdaptiveLoadBalancing/main/data/simulatedData_2.csv'
url = 'https://raw.githubusercontent.com/julianvanmeggelen/AdaptiveLoadBalancing/main/data/simulatedData_4.csv'
df = pd.DataFrame(pd.read_csv(url))
# df = df.drop(columns=['CurrentPeriodNumberOfServers'])

df.info()

df.isnull().sum()

df = df.fillna(0)
df = df.drop(columns=['periodIndex'])

df.describe()

X = df.drop(['nextPeriodReward'], axis = 1)
y = df['nextPeriodReward']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)

scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model = LinearRegression()

model.fit(X_train, y_train)

pred = model.predict(X_test)

print("MAE: ", (metrics.mean_absolute_error(pred, y_test)))
print("MSE: ", (metrics.mean_squared_error(pred, y_test)))
print("R2 score: ", metrics.r2_score(pred, y_test))

sns.regplot(x=pred, y=y_test)
plt.xlabel("Predicted Servers")
plt.ylabel('Actual Servers')
plt.title("Actual vs Predicted Servers")

pickle.dump(model, open('lin_reg_model.pkl', 'wb'))

"""Polynomial Regression"""

transformer = PolynomialFeatures(degree=2, include_bias=False)

X = df.drop(['nextPeriodReward'], axis = 1)
y = df['nextPeriodReward']

x_ = PolynomialFeatures(degree=2, include_bias=False).fit_transform(X)

x_

model= LinearRegression().fit(x_, y)

r_sq = model.score(x_, y)
r_sq

y_pred = model.predict(x_)

print("MAE: ", (metrics.mean_absolute_error(y_pred, y)))
print("MSE: ", (metrics.mean_squared_error(y_pred, y)))
print("R2 score: ", metrics.r2_score(y_pred, y))

"""Advanced Linear Regression """

x = df.drop(['nextPeriodOptimalServers','requestWaitingTime',
             'totalInQueue', 'totalTimeInSystem', 'requestCancelled'], axis = 1)
y = df['nextPeriodOptimalServers']

x = sm.add_constant(x)
x

model = sm.OLS(y, x)

results = model.fit()

print(results.summary())

"""Neural Network"""

import tensorflow as tf
from tensorflow import keras

X = df.drop(['nextPeriodReward'], axis = 1)
y = df['nextPeriodReward']
X

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)
X_train = tf.keras.utils.normalize(X_train, axis = 1)
X_test = tf.keras.utils.normalize(X_test, axis = 1)

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))
model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))
model.add(tf.keras.layers.Activation(activation=tf.nn.softmax))

model.compile(
    optimizer = 'adam',
    loss = 'sparse_categorical_crossentropy',
    metrics = ['accuracy']
)

model.fit(X_train, y_train, epochs = 5)

model.evaluate(X_test, y_test)





"""Random Forest"""

X = df.drop(['nextPeriodReward'], axis = 1)
y = df['nextPeriodReward']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.3)

from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor()
model.fit(X_train, y_train)

model.score(X_val, y_val) #validation score

model.score(X_test, y_test) #test score (end accuracy)







#epsilon-greedy
def epsilon_greedy(min_nServers, max_nServers, prediction):
  epsilon = np.random.binomial(n=1, p=0.5)
  if epsilon == 1:
    nServers = np.random.uniform(low = min_nServers, high = max_nServers)
  else:
    nServers = prediction
  return round(nServers)

#Linear Regression
def init_LR(X_train, y_train):
  model = LinearRegression()
  model.fit(X_train, y_train)
  return model

#Passive Agressive Regression
def init_PAR(X_train, y_train):
  model = linear_model.PassiveAggressiveRegressor()
  model.fit(X_train, y_train)
  return model

#Predict for any model
def predict(model, data):
    pred = model.predict(data)
    return pred

model = init_PAR(X_train[0:15], y_train[0:15])
pred = round(predict(model, [X_test[0]])[0])
print('pred:', pred)
nServers = epsilon_greedy(10, 40, pred)
print('nServers:', nServers)

